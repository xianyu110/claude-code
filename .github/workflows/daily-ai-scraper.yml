name: 📱 小红书AI话题每日采集

on:
  # 定时运行：每天北京时间上午9点（UTC时间凌晨1点）
  schedule:
    - cron: '0 1 * * *'
  
  # 允许手动触发
  workflow_dispatch:
    inputs:
      mode:
        description: '运行模式'
        required: true
        default: 'run'
        type: choice
        options:
        - run
        - test
        - upload

  # 推送到main分支时触发（仅用于测试）
  push:
    branches: [ main ]
    paths:
      - '.github/workflows/daily-ai-scraper.yml'
      - '*.js'

env:
  NODE_ENV: production
  TZ: Asia/Shanghai

jobs:
  scrape-ai-topics:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
    - name: 📥 检出代码
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
    
    - name: 🔧 设置Node.js环境
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'
    
    - name: 📦 安装依赖
      run: |
        npm ci
        npx playwright install chromium
    
    - name: 🔑 配置环境变量
      run: |
        echo "HEADLESS=true" >> $GITHUB_ENV
        echo "GITHUB_ACTIONS=true" >> $GITHUB_ENV
    
    - name: 📂 创建数据目录
      run: |
        mkdir -p data
        
    - name: 📥 下载历史数据
      continue-on-error: true
      run: |
        # 从仓库中恢复历史数据（如果存在）
        if [ -f "data-backup/master_topics.json" ]; then
          cp data-backup/* data/ || true
        fi
    
    - name: 🤖 运行AI话题采集
      id: scraping
      run: |
        MODE="${{ github.event.inputs.mode || 'run' }}"
        echo "运行模式: $MODE"
        
        if [ "$MODE" = "test" ]; then
          node daily_ai_scraper.js test
        elif [ "$MODE" = "upload" ]; then
          node daily_ai_scraper.js upload
        else
          node daily_ai_scraper.js run
        fi
      env:
        FEISHU_TABLE_URL: ${{ secrets.FEISHU_TABLE_URL }}
        FEISHU_APP_ID: ${{ secrets.FEISHU_APP_ID }}
        FEISHU_APP_SECRET: ${{ secrets.FEISHU_APP_SECRET }}
    
    - name: 📊 生成运行报告
      if: always()
      run: |
        echo "## 🤖 小红书AI话题采集报告" > run-report.md
        echo "" >> run-report.md
        echo "**运行时间:** $(date)" >> run-report.md
        echo "**运行模式:** ${{ github.event.inputs.mode || 'auto' }}" >> run-report.md
        echo "" >> run-report.md
        
        if [ -f "data/daily_scraper.log" ]; then
          echo "### 📋 运行日志" >> run-report.md
          echo "\`\`\`" >> run-report.md
          tail -20 data/daily_scraper.log >> run-report.md
          echo "\`\`\`" >> run-report.md
        fi
        
        if [ -f "data/daily_report_$(date +%Y-%m-%d).md" ]; then
          echo "" >> run-report.md
          echo "### 📈 每日数据报告" >> run-report.md
          cat "data/daily_report_$(date +%Y-%m-%d).md" >> run-report.md
        fi
    
    - name: 💾 备份数据文件
      if: always()
      run: |
        # 备份重要数据文件到仓库
        mkdir -p data-backup
        if [ -f "data/master_topics.json" ]; then
          cp data/master_topics.json data-backup/
        fi
        if [ -f "data/scraping_history.json" ]; then
          cp data/scraping_history.json data-backup/
        fi
        if [ -f "feishu_config.json" ]; then
          cp feishu_config.json data-backup/
        fi
    
    - name: 📤 上传运行产物
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: scraping-results-${{ github.run_number }}
        path: |
          data/
          run-report.md
        retention-days: 30
    
    - name: 📧 发送通知（成功）
      if: success()
      run: |
        echo "✅ AI话题采集任务成功完成"
        if [ -f "data/daily_scraper.log" ]; then
          echo "最新日志:"
          tail -5 data/daily_scraper.log
        fi
    
    - name: 📧 发送通知（失败）
      if: failure()
      run: |
        echo "❌ AI话题采集任务失败"
        if [ -f "data/daily_scraper.log" ]; then
          echo "错误日志:"
          tail -10 data/daily_scraper.log
        fi
    
    - name: 🔄 提交数据更新
      if: success() && github.event_name == 'schedule'
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
        # 只提交数据备份文件
        git add data-backup/
        
        if ! git diff --staged --quiet; then
          git commit -m "🤖 自动更新AI话题数据 - $(date '+%Y-%m-%d %H:%M')"
          git push
        else
          echo "没有数据更新需要提交"
        fi

  # 数据处理和分析作业（可选）
  analyze-data:
    runs-on: ubuntu-latest
    needs: scrape-ai-topics
    if: success() && github.event_name == 'schedule'
    
    steps:
    - name: 📥 检出代码
      uses: actions/checkout@v4
    
    - name: 📥 下载采集结果
      uses: actions/download-artifact@v4
      with:
        name: scraping-results-${{ github.run_number }}
        path: ./
    
    - name: 📊 生成趋势分析
      run: |
        echo "## 📈 AI话题趋势分析" > trend-analysis.md
        echo "" >> trend-analysis.md
        echo "**分析时间:** $(date)" >> trend-analysis.md
        echo "" >> trend-analysis.md
        
        if [ -f "data/master_topics.json" ]; then
          echo "### 🔥 热门话题统计" >> trend-analysis.md
          # 这里可以添加更复杂的数据分析逻辑
          echo "数据文件已生成，包含采集的AI话题信息" >> trend-analysis.md
        fi
    
    - name: 📤 上传分析结果
      uses: actions/upload-artifact@v4
      with:
        name: trend-analysis-${{ github.run_number }}
        path: trend-analysis.md 