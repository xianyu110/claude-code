name: ğŸ“± å°çº¢ä¹¦AIè¯é¢˜æ¯æ—¥é‡‡é›†

on:
  # å®šæ—¶è¿è¡Œï¼šæ¯å¤©åŒ—äº¬æ—¶é—´ä¸Šåˆ9ç‚¹ï¼ˆUTCæ—¶é—´å‡Œæ™¨1ç‚¹ï¼‰
  schedule:
    - cron: '0 1 * * *'
  
  # å…è®¸æ‰‹åŠ¨è§¦å‘
  workflow_dispatch:
    inputs:
      mode:
        description: 'è¿è¡Œæ¨¡å¼'
        required: true
        default: 'run'
        type: choice
        options:
        - run
        - test
        - upload

  # æ¨é€åˆ°mainåˆ†æ”¯æ—¶è§¦å‘ï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰
  push:
    branches: [ main ]
    paths:
      - '.github/workflows/daily-ai-scraper.yml'
      - '*.js'

env:
  NODE_ENV: production
  TZ: Asia/Shanghai

jobs:
  scrape-ai-topics:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
    - name: ğŸ“¥ æ£€å‡ºä»£ç 
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
    
    - name: ğŸ”§ è®¾ç½®Node.jsç¯å¢ƒ
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'
    
    - name: ğŸ“¦ å®‰è£…ä¾èµ–
      run: |
        npm ci
        npx playwright install chromium
    
    - name: ğŸ”‘ é…ç½®ç¯å¢ƒå˜é‡
      run: |
        echo "HEADLESS=true" >> $GITHUB_ENV
        echo "GITHUB_ACTIONS=true" >> $GITHUB_ENV
    
    - name: ğŸ“‚ åˆ›å»ºæ•°æ®ç›®å½•
      run: |
        mkdir -p data
        
    - name: ğŸ“¥ ä¸‹è½½å†å²æ•°æ®
      continue-on-error: true
      run: |
        # ä»ä»“åº“ä¸­æ¢å¤å†å²æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if [ -f "data-backup/master_topics.json" ]; then
          cp data-backup/* data/ || true
        fi
    
    - name: ğŸ¤– è¿è¡ŒAIè¯é¢˜é‡‡é›†
      id: scraping
      run: |
        MODE="${{ github.event.inputs.mode || 'run' }}"
        echo "è¿è¡Œæ¨¡å¼: $MODE"
        
        if [ "$MODE" = "test" ]; then
          node daily_ai_scraper.js test
        elif [ "$MODE" = "upload" ]; then
          node daily_ai_scraper.js upload
        else
          node daily_ai_scraper.js run
        fi
      env:
        FEISHU_TABLE_URL: ${{ secrets.FEISHU_TABLE_URL }}
        FEISHU_APP_ID: ${{ secrets.FEISHU_APP_ID }}
        FEISHU_APP_SECRET: ${{ secrets.FEISHU_APP_SECRET }}
    
    - name: ğŸ“Š ç”Ÿæˆè¿è¡ŒæŠ¥å‘Š
      if: always()
      run: |
        echo "## ğŸ¤– å°çº¢ä¹¦AIè¯é¢˜é‡‡é›†æŠ¥å‘Š" > run-report.md
        echo "" >> run-report.md
        echo "**è¿è¡Œæ—¶é—´:** $(date)" >> run-report.md
        echo "**è¿è¡Œæ¨¡å¼:** ${{ github.event.inputs.mode || 'auto' }}" >> run-report.md
        echo "" >> run-report.md
        
        if [ -f "data/daily_scraper.log" ]; then
          echo "### ğŸ“‹ è¿è¡Œæ—¥å¿—" >> run-report.md
          echo "\`\`\`" >> run-report.md
          tail -20 data/daily_scraper.log >> run-report.md
          echo "\`\`\`" >> run-report.md
        fi
        
        if [ -f "data/daily_report_$(date +%Y-%m-%d).md" ]; then
          echo "" >> run-report.md
          echo "### ğŸ“ˆ æ¯æ—¥æ•°æ®æŠ¥å‘Š" >> run-report.md
          cat "data/daily_report_$(date +%Y-%m-%d).md" >> run-report.md
        fi
    
    - name: ğŸ’¾ å¤‡ä»½æ•°æ®æ–‡ä»¶
      if: always()
      run: |
        # å¤‡ä»½é‡è¦æ•°æ®æ–‡ä»¶åˆ°ä»“åº“
        mkdir -p data-backup
        if [ -f "data/master_topics.json" ]; then
          cp data/master_topics.json data-backup/
        fi
        if [ -f "data/scraping_history.json" ]; then
          cp data/scraping_history.json data-backup/
        fi
        if [ -f "feishu_config.json" ]; then
          cp feishu_config.json data-backup/
        fi
    
    - name: ğŸ“¤ ä¸Šä¼ è¿è¡Œäº§ç‰©
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: scraping-results-${{ github.run_number }}
        path: |
          data/
          run-report.md
        retention-days: 30
    
    - name: ğŸ“§ å‘é€é€šçŸ¥ï¼ˆæˆåŠŸï¼‰
      if: success()
      run: |
        echo "âœ… AIè¯é¢˜é‡‡é›†ä»»åŠ¡æˆåŠŸå®Œæˆ"
        if [ -f "data/daily_scraper.log" ]; then
          echo "æœ€æ–°æ—¥å¿—:"
          tail -5 data/daily_scraper.log
        fi
    
    - name: ğŸ“§ å‘é€é€šçŸ¥ï¼ˆå¤±è´¥ï¼‰
      if: failure()
      run: |
        echo "âŒ AIè¯é¢˜é‡‡é›†ä»»åŠ¡å¤±è´¥"
        if [ -f "data/daily_scraper.log" ]; then
          echo "é”™è¯¯æ—¥å¿—:"
          tail -10 data/daily_scraper.log
        fi
    
    - name: ğŸ”„ æäº¤æ•°æ®æ›´æ–°
      if: success() && github.event_name == 'schedule'
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
        # åªæäº¤æ•°æ®å¤‡ä»½æ–‡ä»¶
        git add data-backup/
        
        if ! git diff --staged --quiet; then
          git commit -m "ğŸ¤– è‡ªåŠ¨æ›´æ–°AIè¯é¢˜æ•°æ® - $(date '+%Y-%m-%d %H:%M')"
          git push
        else
          echo "æ²¡æœ‰æ•°æ®æ›´æ–°éœ€è¦æäº¤"
        fi

  # æ•°æ®å¤„ç†å’Œåˆ†æä½œä¸šï¼ˆå¯é€‰ï¼‰
  analyze-data:
    runs-on: ubuntu-latest
    needs: scrape-ai-topics
    if: success() && github.event_name == 'schedule'
    
    steps:
    - name: ğŸ“¥ æ£€å‡ºä»£ç 
      uses: actions/checkout@v4
    
    - name: ğŸ“¥ ä¸‹è½½é‡‡é›†ç»“æœ
      uses: actions/download-artifact@v4
      with:
        name: scraping-results-${{ github.run_number }}
        path: ./
    
    - name: ğŸ“Š ç”Ÿæˆè¶‹åŠ¿åˆ†æ
      run: |
        echo "## ğŸ“ˆ AIè¯é¢˜è¶‹åŠ¿åˆ†æ" > trend-analysis.md
        echo "" >> trend-analysis.md
        echo "**åˆ†ææ—¶é—´:** $(date)" >> trend-analysis.md
        echo "" >> trend-analysis.md
        
        if [ -f "data/master_topics.json" ]; then
          echo "### ğŸ”¥ çƒ­é—¨è¯é¢˜ç»Ÿè®¡" >> trend-analysis.md
          # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„æ•°æ®åˆ†æé€»è¾‘
          echo "æ•°æ®æ–‡ä»¶å·²ç”Ÿæˆï¼ŒåŒ…å«é‡‡é›†çš„AIè¯é¢˜ä¿¡æ¯" >> trend-analysis.md
        fi
    
    - name: ğŸ“¤ ä¸Šä¼ åˆ†æç»“æœ
      uses: actions/upload-artifact@v4
      with:
        name: trend-analysis-${{ github.run_number }}
        path: trend-analysis.md 